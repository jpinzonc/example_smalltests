{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Word to Word example__\n",
    "\n",
    "Code from https://datascience.stackexchange.com/questions/40038/how-to-implement-word-to-word-co-occurence-matrix-in-python\n",
    "\n",
    "Data from https://archive.ics.uci.edu/ml/datasets/Amazon+Commerce+reviews+set#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from itertools import combinations\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like</th>\n",
       "      <th>more</th>\n",
       "      <th>all</th>\n",
       "      <th>invite</th>\n",
       "      <th>crush</th>\n",
       "      <th>not</th>\n",
       "      <th>his</th>\n",
       "      <th>days</th>\n",
       "      <th>friends</th>\n",
       "      <th>do</th>\n",
       "      <th>to</th>\n",
       "      <th>play</th>\n",
       "      <th>smart</th>\n",
       "      <th>krayyem</th>\n",
       "      <th>coffe</th>\n",
       "      <th>is</th>\n",
       "      <th>then</th>\n",
       "      <th>candy</th>\n",
       "      <th>plays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invite</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crush</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friends</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smart</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>krayyem</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coffe</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plays</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         like  more  all  invite  crush  not  his  days  friends   do   to  \\\n",
       "like      3.0   0.0  0.0     0.0    1.0  0.0  0.0   0.0      0.0  0.0  0.0   \n",
       "more      0.0   3.0  0.0     0.0    2.0  0.0  0.0   0.0      0.0  0.0  0.0   \n",
       "all       0.0   0.0  3.0     0.0    2.0  0.0  0.0   2.0      0.0  0.0  0.0   \n",
       "invite    0.0   0.0  0.0     3.0    0.0  2.0  2.0   0.0      1.0  1.0  0.0   \n",
       "crush     1.0   2.0  2.0     0.0    9.0  0.0  0.0   1.0      0.0  0.0  0.0   \n",
       "not       0.0   0.0  0.0     2.0    0.0  3.0  1.0   0.0      0.0  2.0  0.0   \n",
       "his       0.0   0.0  0.0     2.0    0.0  1.0  3.0   0.0      2.0  0.0  1.0   \n",
       "days      0.0   0.0  2.0     0.0    1.0  0.0  0.0   3.0      0.0  1.0  0.0   \n",
       "friends   0.0   0.0  0.0     1.0    0.0  0.0  2.0   0.0      3.0  0.0  2.0   \n",
       "do        0.0   0.0  0.0     1.0    0.0  2.0  0.0   1.0      0.0  3.0  0.0   \n",
       "to        0.0   0.0  0.0     0.0    0.0  0.0  1.0   0.0      2.0  0.0  3.0   \n",
       "play      0.0   0.0  0.0     0.0    1.0  0.0  0.0   0.0      1.0  0.0  2.0   \n",
       "smart     0.0   0.0  0.0     0.0    0.0  0.0  0.0   0.0      0.0  0.0  0.0   \n",
       "krayyem   2.0   0.0  1.0     0.0    2.0  1.0  0.0   2.0      0.0  2.0  0.0   \n",
       "coffe     0.0   1.0  0.0     0.0    0.0  0.0  0.0   0.0      0.0  0.0  0.0   \n",
       "is        0.0   0.0  0.0     0.0    1.0  0.0  0.0   0.0      0.0  0.0  0.0   \n",
       "then      0.0   2.0  0.0     0.0    1.0  0.0  0.0   0.0      0.0  0.0  0.0   \n",
       "candy     2.0   1.0  1.0     0.0    6.0  0.0  0.0   0.0      0.0  0.0  1.0   \n",
       "plays     0.0   0.0  0.0     0.0    1.0  0.0  0.0   0.0      0.0  0.0  0.0   \n",
       "\n",
       "         play  smart  krayyem  coffe   is  then  candy  plays  \n",
       "like      0.0    0.0      2.0    0.0  0.0   0.0    2.0    0.0  \n",
       "more      0.0    0.0      0.0    1.0  0.0   2.0    1.0    0.0  \n",
       "all       0.0    0.0      1.0    0.0  0.0   0.0    1.0    0.0  \n",
       "invite    0.0    0.0      0.0    0.0  0.0   0.0    0.0    0.0  \n",
       "crush     1.0    0.0      2.0    0.0  1.0   1.0    6.0    1.0  \n",
       "not       0.0    0.0      1.0    0.0  0.0   0.0    0.0    0.0  \n",
       "his       0.0    0.0      0.0    0.0  0.0   0.0    0.0    0.0  \n",
       "days      0.0    0.0      2.0    0.0  0.0   0.0    0.0    0.0  \n",
       "friends   1.0    0.0      0.0    0.0  0.0   0.0    0.0    0.0  \n",
       "do        0.0    0.0      2.0    0.0  0.0   0.0    0.0    0.0  \n",
       "to        2.0    0.0      0.0    0.0  0.0   0.0    1.0    0.0  \n",
       "play      3.0    0.0      0.0    0.0  0.0   0.0    2.0    0.0  \n",
       "smart     0.0    1.0      1.0    0.0  1.0   0.0    0.0    0.0  \n",
       "krayyem   0.0    1.0     12.0    2.0  2.0   1.0    3.0    2.0  \n",
       "coffe     0.0    0.0      2.0    3.0  0.0   2.0    0.0    1.0  \n",
       "is        0.0    1.0      2.0    0.0  2.0   0.0    0.0    0.0  \n",
       "then      0.0    0.0      1.0    2.0  0.0   3.0    0.0    0.0  \n",
       "candy     2.0    0.0      3.0    0.0  0.0   0.0    9.0    2.0  \n",
       "plays     0.0    0.0      2.0    1.0  0.0   0.0    2.0    3.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ctxs = [\n",
    "    'krayyem like candy crush more then coffe',\n",
    "    'krayyem plays candy crush all days',\n",
    "    'krayyem do not invite his friends to play candy crush',\n",
    "    'krayyem is smart',\n",
    "]\n",
    "\n",
    "l_unique = list(set((' '.join(ctxs)).split(' ')))\n",
    "mat = np.zeros((len(l_unique), len(l_unique)))\n",
    "\n",
    "nei = []\n",
    "nei_size = 3\n",
    "\n",
    "for ctx in ctxs:\n",
    "    for word in ctx.split(' '):\n",
    "        nei.append(word)\n",
    "        if len(nei) > nei_size:\n",
    "            nei.pop(0)\n",
    "        for word_1 in nei:\n",
    "            for word_2 in nei:\n",
    "                # if word_1 != word_2 -> to avoid diagonal\n",
    "                mat[l_unique.index(word_1), l_unique.index(word_2)] += 1\n",
    "\n",
    "mat = pd.DataFrame(mat)\n",
    "mat.index = l_unique\n",
    "mat.columns = l_unique\n",
    "display(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'not', 'to', 'goes', 'do', 'go', 'but', 'london', 'i', 'you'} \n",
      "\n",
      "Each sentence in token form:\n",
      " [['i', 'go', 'to', 'london'], ['you', 'do', 'not', 'go', 'to', 'london'], ['but', 'london', 'goes', 'to', 'you']] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'not': {'to': 1, 'do': 1, 'go': 1, 'you': 1},\n",
       " 'to': {'go': 2, 'london': 3, 'i': 1, 'not': 1, 'goes': 1, 'you': 1},\n",
       " 'goes': {'to': 1, 'but': 1, 'london': 1, 'you': 1},\n",
       " 'do': {'not': 1, 'go': 1, 'you': 1},\n",
       " 'go': {'to': 2, 'london': 2, 'i': 1, 'do': 1, 'not': 1},\n",
       " 'but': {'goes': 1, 'london': 1},\n",
       " 'london': {'to': 3, 'go': 2, 'but': 1, 'goes': 1},\n",
       " 'i': {'to': 1, 'go': 1},\n",
       " 'you': {'not': 1, 'do': 1, 'goes': 1, 'to': 1}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = ['i go to london', 'you do not go to london', 'but london goes to you']\n",
    "vocab = set(word_tokenize(' '.join(sentences)))\n",
    "print('Vocabulary:\\n',vocab,'\\n')\n",
    "token_sent_list = [word_tokenize(sen) for sen in sentences]\n",
    "print('Each sentence in token form:\\n',token_sent_list,'\\n')\n",
    "\n",
    "co_occ = {ii:Counter({jj:0 for jj in vocab if jj!=ii}) for ii in vocab}\n",
    "k=2\n",
    "\n",
    "for sen in token_sent_list:\n",
    "    for ii in range(len(sen)):\n",
    "        if ii < k:\n",
    "            c = Counter(sen[0:ii+k+1])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "        elif ii > len(sen)-(k+1):\n",
    "            c = Counter(sen[ii-k::])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "        else:\n",
    "            c = Counter(sen[ii-k:ii+k+1])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "\n",
    "# Having final matrix in dict form lets you convert it to different python data structures\n",
    "co_occ = {ii:dict(co_occ[ii]) for ii in vocab}\n",
    "display(co_occ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING A DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from: https://data.world/ostp/wsrd-testbed-inventory \n",
    "data_inventory = pd.read_csv('data/Testbed_Inventory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove space at the end of the sentence\n",
    "data_world = data_inventory['name'].str.replace(r' $', '')\n",
    "# Create a list of the sentences\n",
    "text = list(data_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calit2 ericsson wireless access network research testbed', 'afrl aerial layer networking facilities', 'nrl cognitive radio test laboratory', 'orbit open access next generation wireless network testbed', 'spectrum sharing innovation testbed', 'public safety communications research lab pscr', 'cognitive radio network testbed cornet', 'nrl tactical edge network testbed', 'ornl communications testbed', 'electronic proving ground us army test ranges', 'white sands missile range us army test ranges', 'aberdeen test center us army test ranges', 'redstone test center us army test ranges', 'yuma proving ground us army test ranges', 'idaho national laboratory wireless national testbed', 'cmulab', 'd meas learn', 'digital object registry', 'cloudctl vise', 'bgpmux dtunnels', 'enterprisegeni openflow', 'geni4yr', 'gmoc netkarma kgeni', 'gpeni', 'gush proto', 'instools ism infrastructure', 'kansei otm', 'max', 'measurementsys', 'millionnodegeni security', 'orbit wimax', 'orcaben', 'planetlab scaffold federation', 'protogeni', 'provserv', 'erm', 'regopt', 'secarch distributed identity', 'spp', 'tied', 'uboanets', 'umlpen', 'crgeni', 'cront', 'design of information subs', 'dsl hive', 'expsec', 'fpgaradio', 'geni imf', 'igeni', 'lamp', 'lefa supercharged planetlab', 'nlr', 'opencirrus', 'okgems', 'pigeonnet', 'primogeni', 'quilt', 's3geni', 'secpol', 'vmi']\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation \n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in text]\n",
    "# Converting the case to lower\n",
    "lowered_text = [word.lower() for word in  stripped]\n",
    "print(lowered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing STOP WORDS\n",
    "set(stopwords.words('english'))\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('calit2')\n",
    "filtered_sentence = [w for w in lowered_text if not w in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afrl aerial layer networking facilities',\n",
       " 'nrl cognitive radio test laboratory',\n",
       " 'orbit open access next generation wireless network testbed',\n",
       " 'spectrum sharing innovation testbed',\n",
       " 'public safety communications research lab pscr',\n",
       " 'cognitive radio network testbed cornet',\n",
       " 'nrl tactical edge network testbed',\n",
       " 'ornl communications testbed',\n",
       " 'electronic proving ground us army test ranges',\n",
       " 'white sands missile range us army test ranges',\n",
       " 'aberdeen test center us army test ranges',\n",
       " 'redstone test center us army test ranges',\n",
       " 'yuma proving ground us army test ranges',\n",
       " 'idaho national laboratory wireless national testbed',\n",
       " 'cmulab',\n",
       " 'd meas learn',\n",
       " 'digital object registry',\n",
       " 'cloudctl vise',\n",
       " 'bgpmux dtunnels',\n",
       " 'enterprisegeni openflow',\n",
       " 'geni4yr',\n",
       " 'gmoc netkarma kgeni',\n",
       " 'gpeni',\n",
       " 'gush proto',\n",
       " 'instools ism infrastructure',\n",
       " 'kansei otm',\n",
       " 'max',\n",
       " 'measurementsys',\n",
       " 'millionnodegeni security',\n",
       " 'orbit wimax',\n",
       " 'orcaben',\n",
       " 'planetlab scaffold federation',\n",
       " 'protogeni',\n",
       " 'provserv',\n",
       " 'erm',\n",
       " 'regopt',\n",
       " 'secarch distributed identity',\n",
       " 'spp',\n",
       " 'tied',\n",
       " 'uboanets',\n",
       " 'umlpen',\n",
       " 'crgeni',\n",
       " 'cront',\n",
       " 'design of information subs',\n",
       " 'dsl hive',\n",
       " 'expsec',\n",
       " 'fpgaradio',\n",
       " 'geni imf',\n",
       " 'igeni',\n",
       " 'lamp',\n",
       " 'lefa supercharged planetlab',\n",
       " 'nlr',\n",
       " 'opencirrus',\n",
       " 'okgems',\n",
       " 'pigeonnet',\n",
       " 'primogeni',\n",
       " 'quilt',\n",
       " 's3geni',\n",
       " 'secpol',\n",
       " 'vmi']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = filtered_sentence\n",
    "l_unique = list(set((' '.join(text_list)).split(' ')))\n",
    "mat = np.zeros((len(l_unique), len(l_unique)))\n",
    "nei = []\n",
    "nei_size = 3\n",
    "\n",
    "for ctx in text_list:\n",
    "    for word in ctx.split(' '):\n",
    "        nei.append(word)\n",
    "        if len(nei) > nei_size:\n",
    "            nei.pop(0)\n",
    "        for word_1 in nei:\n",
    "            for word_2 in nei:\n",
    "                # if word_1 != word_2 -> to avoid diagonal\n",
    "                mat[l_unique.index(word_1), l_unique.index(word_2)] += 1\n",
    "\n",
    "mat = pd.DataFrame(mat)\n",
    "mat.index = l_unique\n",
    "mat.columns = l_unique\n",
    "display(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = filtered_sentence\n",
    "vocab = set(word_tokenize(' '.join(sentences)))\n",
    "#print('Vocabulary:\\n',vocab,'\\n')\n",
    "token_sent_list = [word_tokenize(sen) for sen in sentences]\n",
    "#print('Each sentence in token form:\\n',token_sent_list,'\\n')\n",
    "\n",
    "co_occ = {ii:Counter({jj:0 for jj in vocab if jj!=ii}) for ii in vocab}\n",
    "k=2\n",
    "\n",
    "for sen in token_sent_list:\n",
    "    for ii in range(len(sen)):\n",
    "        if ii < k:\n",
    "            c = Counter(sen[0:ii+k+1])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "        elif ii > len(sen)-(k+1):\n",
    "            c = Counter(sen[ii-k::])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "        else:\n",
    "            c = Counter(sen[ii-k:ii+k+1])\n",
    "            del c[sen[ii]]\n",
    "            co_occ[sen[ii]] = co_occ[sen[ii]] + c\n",
    "\n",
    "# Having final matrix in dict form lets you convert it to different python data structures\n",
    "co_occ = {ii:dict(co_occ[ii]) for ii in vocab}\n",
    "display(co_occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
